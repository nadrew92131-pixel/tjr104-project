import requests  # 負責：發送網路請求，把網頁內容和檔案「搬」回來
from bs4 import BeautifulSoup  # 負責：解析 HTML 原始碼，像濾網一樣過濾出我們要的資訊
import os  # 負責：跟你的作業系統溝通，建立資料夾、串接路徑、檢查檔案
import urllib3  # 負責：底層網路設定，這裡用來關閉 SSL 安全檢查警告
import zipfile  # 負責：處理壓縮檔，讀取裡面的 CSV 內容
import pandas as pd  # 負責：資料處理，把 CSV 轉成表格，進行切分與清洗
import re #因為近年資料A1,A2的title分別為"CSV下載檔案"以及"ZIP下載檔案",所以用Regular Expression只抓取"下載檔案"
from config import HEADERS,SEQ_PAGE_URL,SAVE_OLD_DATA_DIR,SAVE_NEW_DATA_DIR,RECENT_PAGE_A1_URL,RECENT_PAGE_A2_URL 


# 關閉「不安全請求」的警告，因為我們使用了 verify=False (跳過憑證檢查)
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
# ------------------------------------------------------------------
# 爬蟲與下載 (Extract) .zip
# ------------------------------------------------------------------
def auto_scrape_and_download_old_data()->list:
    # 建立存檔資料夾，exist_ok=True 代表若資料夾已存在就不會報錯
    os.makedirs(SAVE_OLD_DATA_DIR, exist_ok=True)
     
    extracted_data_year = [] #用來放入每年的list
    for page_url in SEQ_PAGE_URL:
        extracted_data_12mon = []# 用來存讀取出來的 12個月的DataFrame
        print(f"正在分析網頁結構: {page_url}")
    # verify=False 解決政府網站常見的 SSL 憑證驗證失敗問題
        res = requests.get(page_url, headers=HEADERS, verify=False)
    
    # 使用 BeautifulSoup 解析 HTML，'html.parser' 是 Python 內建的解析器
        soup = BeautifulSoup(res.text, 'html.parser')
    
    # find() 找尋第一個符合條件的標籤，我們利用 title 屬性精準定位下載按鈕
        csv_link_tag = soup.find('a', title="CSV下載檔案")

        if not csv_link_tag:
            print("錯誤：找不到下載標籤！")
            return False

        FILE_URL = csv_link_tag.get('href') # 取得 <a> 標籤中的 href 屬性 (即檔案網址)
        print(f"成功鎖定檔案位址: {FILE_URL}")
    
        try:
        # stream=True 啟用串流模式，檔案會一塊一塊下載，不會一次塞爆記憶體
            with requests.get(FILE_URL, headers=HEADERS, stream=True, timeout=60, verify=False) as r:
                r.raise_for_status() # 檢查連線狀態，若非 200 (成功) 則觸發異常
                
                
                raw_filename =  FILE_URL.split('/')[-1]#這邊的的raw_filename 是一個接收FILE_URL被split('/')<--最後一個'/' 的物件
                if not raw_filename.endswith('.zip'): #如果沒有抓到上面的規則,就抓網頁的url的倒數'/'當檔名
                    file_id = page_url.split('/')[-1]
                    raw_filename = f"accident_{file_id}.zip"
                #最終路徑組合
                final_path = os.path.join(SAVE_OLD_DATA_DIR, raw_filename)
                print(f"正在下載至: {final_path}")

                with open(final_path, 'wb') as f:
                    # iter_content() 每次讀取 1MB 的資料，並寫入硬碟
                    for chunk in r.iter_content(chunk_size=1024*1024):
                        if chunk:
                         f.write(chunk)
            print(f"【下載成功】檔案存於: {final_path}")

            print(f"--- 解壓縮 ---")
            with zipfile.ZipFile(final_path, 'r') as z:
                for info in z.infolist():
            # 處理檔名亂碼 (cp437 -> cp950)
                    try:
                        real_name = info.filename.encode('cp437').decode('cp950')
                    except:
                        real_name = info.filename
            
            # 只抓 A1、A2 的 CSV
                if ("A1" in real_name or "A2" in real_name) and real_name.endswith('.csv'):
                    with z.open(info.filename) as f:
                    # 讀取並存入 list 中
                        df = pd.read_csv(f, encoding='utf-8-sig')
                        print(f"已讀取檔案: {real_name}")
                        extracted_data_12mon.append(df)

            if extracted_data_12mon:
                extracted_data_year.append(extracted_data_12mon)

        except Exception as e:
            print(f"下載失敗: {e}")
    return extracted_data_year

# ------------------------------------------------------------------
# 爬蟲與下載近期 (Extract) .csv/.zip
# ------------------------------------------------------------------
def auto_scrape_recent_data()->list:
    # 1. 建立資料夾
    os.makedirs(SAVE_NEW_DATA_DIR, exist_ok=True)
    A1A2_List=[]
    # 2. 發送請求並放入清單 (正確的 append 或直接宣告)
    ra1 = requests.get(RECENT_PAGE_A1_URL, headers=HEADERS, verify=False)
    ra2 = requests.get(RECENT_PAGE_A2_URL, headers=HEADERS, verify=False)
    recentA1A2 = [ra1, ra2] 
    
    # 為了區分檔名，我們加一個簡單的計數器
    count = 1

    for r1r2 in recentA1A2:
        soup = BeautifulSoup(r1r2.text, 'html.parser')
        csv_link_tag = soup.find('a', title=re.compile("下載檔案"))
       
        if not csv_link_tag:
            print(f"錯誤：第 {count} 個頁面找不到下載標籤！")
            count += 1
            continue # 找不到就換下一個，不要直接 return False

        FILE_URL = csv_link_tag.get('href')
       
        print(f"成功鎖定檔案位址: {FILE_URL}")
    
        try:
            with requests.get(FILE_URL, headers=HEADERS, stream=True, timeout=60, verify=False) as r:
                r.raise_for_status()

                tag_title = csv_link_tag.get('title', '')
                if "ZIP" in tag_title:
                    file_name = f"recent_A{count}.zip"
                elif "CSV" in tag_title:
                    file_name = f"recent_A{count}.csv"

                final_save_path = os.path.join(SAVE_NEW_DATA_DIR, file_name)

                print(f"正在下載至: {final_save_path}")


                with open(final_save_path, 'wb') as f:
                    for chunk in r.iter_content(chunk_size=1024*1024):
                        if chunk:
                            f.write(chunk)

                if ".zip" in file_name:
                    with zipfile.ZipFile(final_save_path, 'r') as z:
                        for info in z.infolist():
                        # 處理檔名亂碼 (cp437 -> cp950)
                            try:
                                real_name = info.filename.encode('cp437').decode('cp950')
                            except:
                                real_name = info.filename
            
                    # 只抓 A1、A2 的 CSV
                        if ("A1" in real_name or "A2" in real_name) and real_name.endswith('.csv'):
                            with z.open(info.filename) as f:
                        # 讀取並存入 list 中
                                df = pd.read_csv(f, encoding='utf-8-sig')
                        print(f"已讀取檔案: {real_name}")
                elif ".csv" in file_name: 
                    df = pd.read_csv(final_save_path,encoding='utf-8-sig')


                #print(df.head())
                A1A2_List.append(df)
                print(f"【下載成功】檔案存於: {final_save_path}")
                count += 1 # 檔名序號加一
     

        except Exception as e:
            print(f"下載失敗: {e}")
            
    return A1A2_List




